{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f826824-d452-46dc-8eb1-bfcad82c8085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydantic==1.10.8\n",
      "  Downloading pydantic-1.10.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic==1.10.8) (4.4.0)\n",
      "Installing collected packages: pydantic\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.9.2\n",
      "    Uninstalling pydantic-1.9.2:\n",
      "      Successfully uninstalled pydantic-1.9.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spacy 3.4.1 requires pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4, but you have pydantic 1.10.8 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pydantic-1.10.8\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U pydantic==1.10.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "582a2f5b-7fe8-4442-ab63-871f0a5ea2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.0.276-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting numexpr<3.0.0,>=2.8.4\n",
      "  Downloading numexpr-2.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (382 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m382.8/382.8 kB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.21\n",
      "  Downloading langsmith-0.0.27-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.23.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.10.8)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7\n",
      "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.9/dist-packages (from langchain) (2.28.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.9/dist-packages (from langchain) (3.8.3)\n",
      "Collecting tenacity<9.0.0,>=8.1.0\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.9/dist-packages (from langchain) (5.4.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.4.41)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (18.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspect<1,>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic<3,>=1->langchain) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain) (2.8)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.9/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.0)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: tenacity, numexpr, mypy-extensions, marshmallow, typing-inspect, langsmith, dataclasses-json, langchain\n",
      "  Attempting uninstall: marshmallow\n",
      "    Found existing installation: marshmallow 2.21.0\n",
      "    Uninstalling marshmallow-2.21.0:\n",
      "      Successfully uninstalled marshmallow-2.21.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradient 2.0.6 requires marshmallow<3.0, but you have marshmallow 3.20.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed dataclasses-json-0.5.14 langchain-0.0.276 langsmith-0.0.27 marshmallow-3.20.1 mypy-extensions-1.0.0 numexpr-2.8.5 tenacity-8.2.3 typing-inspect-0.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0056c248-8259-403b-bcc2-e9bfb29a2bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-0.27.9-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from openai) (3.8.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.28.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.20->openai) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.20->openai) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.1.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (18.2.0)\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-0.27.9\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88ce9f20-a15a-46e9-b744-908ced68bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "concept = '''\n",
    "Gradient Descent is an iterative optimization process that searches for an objective function’s\n",
    " optimum value (Minimum/Maximum). It is one of the most used methods for changing a model’s parameters in\n",
    " order to reduce a cost function in machine learning projects.  \n",
    "The primary goal of gradient descent is to identify the model parameters that provide the maximum accuracy\n",
    " on both training and test datasets. In gradient descent, the gradient is a vector pointing in the general\n",
    " direction of the function’s steepest rise at a particular point. The algorithm might gradually drop towards\n",
    " lower values of the function by moving in the opposite direction of the gradient, until reaching the minimum\n",
    " of the function.\n",
    " '''\n",
    "student_own_words='''\n",
    "Gradient descent is an optimization algorithm that is commonly used to minimize cost functions in\n",
    " machine learning models. It works by iteratively adjusting the model parameters, like the weights and biases\n",
    " in a neural network, in the direction that reduces the cost function.\n",
    "\n",
    "The goal is to converge on the optimal set of parameters that minimize the cost, like the error between\n",
    " predictions and true labels. Gradient descent starts with random initial parameters, then calculates the gradient\n",
    " of the cost function. The gradient tells you which direction to update the parameters to reduce the cost.\n",
    "The parameters are updated by a small amount in the negative gradient direction. This process is repeated until\n",
    "the algorithm converges on a minimum cost.\n",
    "\n",
    "So in summary, gradient descent iteratively fine-tunes the model parameters by calculating the gradient and moving\n",
    " in the direction that reduces the cost function. By repeating this process, it can find the optimal parameters that\n",
    " minimize the cost and maximize model accuracy.\n",
    " '''\n",
    "template =  '''\n",
    "    Read the following concept and a student's in-your-own-words description and evaluate the student's\n",
    "description by the following criteria: \n",
    "- coverage of the core aspects of the concept\n",
    "- clarity and simplicity of the explanation\n",
    "- Identify gaps in understanding and areas that need improvement\n",
    "\n",
    "Provide a score on how well the concept was explained.\n",
    "\n",
    "Concept: `{concept}`\n",
    "\n",
    "Student: `{student_own_words}`\n",
    "Evaluation:\n",
    "'''\n",
    "prompt_template= PromptTemplate(template=template, input_variables=[\"concept\", \"student_own_words\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3fbedc5-5762-4c1b-a719-283bf8f7dae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "OPENAI_API_KEY=getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4877d424-f182-4eac-85f6-7eef02d53abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9780de9e-4b3f-455d-905b-7a8ae4fd1f00",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d2321ec-df2d-48ac-bfc4-234e3b65f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3467a3e2-484f-4a2e-9f69-306c6d35b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "openai_llm_chain = LLMChain(llm=openai_llm,prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e99577a-82e4-424f-9a8e-332937d935e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai_llm_chain.predict(concept=concept, student_own_words=student_own_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b57e3c0-3ff9-4005-8c76-40b4d3411429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The student's explanation of the concept of Gradient Descent is quite good. The student covers the core aspects of the concept including how it is an iterative optimization algorithm used to minimize cost functions, how the model parameters are adjusted, and how the gradient of the cost function is used to determine the direction of the parameter updates. \n",
      "\n",
      "The explanation is clear and simple, using understandable language and providing a summary at the end to reinforce the main points. \n",
      "\n",
      "However, the student does not mention that gradient descent can also be used to maximize a function, not just minimize it. This is a small point but it shows a slight gap in understanding. The student could also provide a more detailed explanation of what the 'gradient' is and how it relates to the 'steepest rise' of a function.\n",
      "\n",
      "Score: 8/10. Overall, the student has a good understanding of Gradient Descent, but there are a couple of points that could be improved upon.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3ed0aab-11af-46a0-ac64-52e14e9fe4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The student did not provide any information related to the concept of Gradient Descent. It seems like they misunderstood or mistyped the concept. Therefore, it's not possible to evaluate the coverage, clarity, simplicity, understanding or areas that need improvement regarding the Gradient Descent concept. \n",
      "\n",
      "Score: 0/10.\n"
     ]
    }
   ],
   "source": [
    "zero_response = openai_llm_chain.predict(concept=concept, student_own_words=\"climate change\")\n",
    "print(zero_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a139a2-c932-428c-8fda-61e5a1b4cddb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GPT-3.5-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0db974e8-fb6d-48b8-a828-181877a6402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb37393e-4851-40c7-a1ca-b9adff6aa644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "openai_llm_chain = LLMChain(llm=openai_llm,prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bc01e33-e5e0-462d-afe9-799c715e53a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai_llm_chain.predict(concept=concept, student_own_words=student_own_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02dcdaa6-51cf-4edf-931c-4b2ccd8bd793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The student's description of gradient descent covers the core aspects of the concept. They mention that it is an optimization algorithm used to minimize cost functions in machine learning models and that it iteratively adjusts model parameters in the direction that reduces the cost function. They also explain that the goal is to converge on the optimal set of parameters that minimize the cost.\n",
      "\n",
      "The explanation is clear and simple, using straightforward language to describe the process of gradient descent. The student uses examples, such as weights and biases in a neural network, to help illustrate the concept.\n",
      "\n",
      "One gap in understanding is that the student does not mention the concept of the gradient being a vector pointing in the direction of the function's steepest rise at a particular point. This is an important aspect of gradient descent that helps guide the algorithm towards the minimum of the function.\n",
      "\n",
      "Another area that could be improved is the explanation of how the algorithm converges on a minimum cost. The student briefly mentions that the parameters are updated by a small amount in the negative gradient direction, but does not go into detail about how the algorithm determines when it has reached a minimum.\n",
      "\n",
      "Overall, the student's explanation of gradient descent is clear and covers the core aspects of the concept. However, there are some gaps in understanding and areas that could be improved. \n",
      "\n",
      "Score: 3.5/5\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "081b6621-e3bf-4339-8d99-e797475db4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The student's description does not cover the core aspects of the concept of gradient descent. They only mention \"climate change,\" which is not related to the concept at all. The explanation lacks clarity and simplicity as it does not provide any information or understanding of the concept. There are significant gaps in understanding, as the student does not address any of the key points mentioned in the concept description. \n",
      "\n",
      "Score: 0/10\n"
     ]
    }
   ],
   "source": [
    "zero_response = openai_llm_chain.predict(concept=concept, student_own_words=\"climate change\")\n",
    "print(zero_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90427ed-48df-4b38-87de-a96f7895f348",
   "metadata": {},
   "source": [
    "# LaMini-Flan-T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0866e7a-7420-4e3a-b752-cb4e7ff332b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3f2eb68ea84590a8670d8d0304572f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/2.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device has 1 GPUs available. Provide device={deviceId} to `from_model_id` to use availableGPUs for execution. deviceId is -1 (default) for CPU and can be a positive integer associated with CUDA device id.\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "model_id = \"MBZUAI/LaMini-Flan-T5-783M\" \n",
    "flan_llm = HuggingFacePipeline.from_model_id(model_id=model_id, task=\"text2text-generation\", model_kwargs={\"temperature\":0, \"max_length\":4096})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80ac4c50-ceb2-4f87-a4ff-4a0d73b56d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "flan_llm_chain = LLMChain(llm=flan_llm,prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96b863bd-64c4-4080-889f-e8ff584b07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = flan_llm_chain.predict(concept=concept, student_own_words=student_own_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf89a849-80b6-4605-8c84-47f006aba594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The student's description is clear and concise in its explanation of the concept of gradient descent.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5bf9f7c-74f4-41d8-b97e-f57467b41aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The student's description is clear and concise in explaining the concept of gradient descent. However, there are some gaps in understanding and areas that need improvement. Overall, the student's description is well-intentioned and provides a clear explanation of the concept.\n"
     ]
    }
   ],
   "source": [
    "zero_response = flan_llm_chain.predict(concept=concept, student_own_words=\"climate change\")\n",
    "print(zero_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a61034-789d-4148-9889-00cabdd6ef9d",
   "metadata": {},
   "source": [
    "# LaMini-Neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "346b96ef-b2fa-40ab-88a5-726c7e609b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2abe4399874acf9eddf972e24d45e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/788 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5623e66659643ba80e00662bd44413c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/779k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25576b7688f44e4c810a43e3f229b92f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14d44ce71454a0e955637c4cd798419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.01M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3ff98f574945128cd74f1233bba058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e67b043c9c74b078a24b4b086d5760e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50654b6ef28f49be8d89ac4347525d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ff6e9f35b54a378fc29682e69553aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device has 1 GPUs available. Provide device={deviceId} to `from_model_id` to use availableGPUs for execution. deviceId is -1 (default) for CPU and can be a positive integer associated with CUDA device id.\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "model_id = \"MBZUAI/LaMini-Neo-1.3B\" \n",
    "neo_llm = HuggingFacePipeline.from_model_id(model_id=model_id, task=\"text-generation\", model_kwargs={\"temperature\":0.5, \"max_length\":4096})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "598a0026-bac7-4cbc-92ab-0ff7028e578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "neo_llm_chain = LLMChain(llm=neo_llm,prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "757b570f-60f1-46ed-8f0a-4ebb8165d74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation_utils.py:1202: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 50 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Input length of input_ids is 441, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n"
     ]
    }
   ],
   "source": [
    "response = neo_llm_chain.predict(concept=concept, student_own_words=student_own_words)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f43c7cb6-d8af-46de-ac5e-b16e2caf4afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 244, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n"
     ]
    }
   ],
   "source": [
    "zero_response = neo_llm_chain.predict(concept=concept, student_own_words=\"climate change\")\n",
    "print(zero_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1cc746-c515-4114-904b-01e098d519ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
